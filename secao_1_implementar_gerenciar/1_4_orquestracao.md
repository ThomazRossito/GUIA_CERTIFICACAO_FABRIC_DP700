# 1.4 Orquestrar processos

A orquestra√ß√£o de processos no Microsoft Fabric refere-se √† **automa√ß√£o, sequenciamento e gerenciamento de ponta a ponta** das tarefas de engenharia de dados. Um pipeline de dados bem orquestrado garante que os dados sejam ingeridos, transformados e disponibilizados para an√°lise de forma confi√°vel, eficiente, escal√°vel e no tempo certo. O Fabric oferece v√°rias ferramentas poderosas para construir e gerenciar esses fluxos de trabalho complexos.

## Vis√£o Geral da Orquestra√ß√£o no Fabric

```mermaid
graph TD
    A[Fontes de Dados] --> B{Escolher Ferramenta}
    
    B -->|Transforma√ß√£o Low-Code| C[Dataflow Gen2]
    B -->|Orquestra√ß√£o Complexa| D[Pipeline]
    B -->|Transforma√ß√£o Code-First| E[Notebook]
    
    C --> F[Lakehouse/Warehouse]
    D --> G[Coordena M√∫ltiplas Atividades]
    E --> F
    
    G --> C
    G --> E
    G --> H[Copy Activity]
    G --> I[Stored Procedure]
    
    style B fill:#fff4e1
    style C fill:#e8f5e9
    style D fill:#e3f2fd
    style E fill:#f3e5f5
```

---

## 1. Escolhendo entre Dataflow Gen2, Pipeline e Notebook

A escolha da ferramenta de orquestra√ß√£o correta √© **cr√≠tica** para o sucesso do projeto e depende da natureza da tarefa, complexidade da transforma√ß√£o e prefer√™ncia de desenvolvimento.

### 1.1 Compara√ß√£o Detalhada

| Aspecto | Dataflow Gen2 | Pipeline | Notebook |
|---------|---------------|----------|----------|
| **Abordagem** | Low-Code (Visual) | Orquestra√ß√£o (Visual + Code) | Code-First (PySpark, SQL, Scala) |
| **Interface** | Power Query Online | Pipeline Designer | Notebook Editor |
| **Linguagem** | Power Query M | Express√µes JSON | PySpark, Spark SQL, Scala, R |
| **Melhor para** | Transforma√ß√µes simples a m√©dias | Orquestra√ß√£o de m√∫ltiplas atividades | Transforma√ß√µes complexas, ML |
| **Complexidade** | Baixa a M√©dia | M√©dia a Alta | Alta |
| **Curva de Aprendizado** | Baixa | M√©dia | Alta |
| **Performance** | Boa para datasets pequenos/m√©dios | N/A (orquestra outras ferramentas) | Excelente (Spark distribu√≠do) |
| **Destinos** | Lakehouse, Warehouse | N/A (executa atividades) | Lakehouse, Warehouse, OneLake |
| **Parametriza√ß√£o** | Sim (par√¢metros do Dataflow) | Sim (par√¢metros do Pipeline) | Sim (widgets, argumentos) |
| **Agendamento** | Sim (refresh schedule) | Sim (triggers) | Via Pipeline |
| **Debugging** | Query diagnostics | Pipeline run history | Spark UI, logs |

### 1.2 Quando Usar Cada Ferramenta

#### Dataflow Gen2

**‚úÖ Use quando**:
- Transforma√ß√µes podem ser feitas com Power Query M
- Equipe prefere interface visual
- Dados cabem em mem√≥ria (< 1 GB recomendado)
- Transforma√ß√µes s√£o ETL padr√£o (filtros, joins, agrega√ß√µes)
- Precisa de query folding para performance

**‚ùå Evite quando**:
- Dados muito grandes (> 10 GB)
- Transforma√ß√µes complexas que requerem c√≥digo customizado
- Precisa de controle fino sobre particionamento Spark
- L√≥gica de neg√≥cio muito complexa

**Exemplo de Uso**:
```
Cen√°rio: Limpar e transformar dados de vendas de um arquivo CSV
Ferramenta: Dataflow Gen2
Raz√£o: Transforma√ß√µes simples (remover duplicatas, filtrar, agregar)
```

#### Pipeline

**‚úÖ Use quando**:
- Precisa orquestrar m√∫ltiplas atividades
- Workflow tem depend√™ncias complexas
- Precisa copiar dados de/para m√∫ltiplas fontes
- Combina diferentes tipos de processamento (Dataflow + Notebook + SP)
- Precisa de error handling e retry logic
- Workflow tem condicionais e loops

**‚ùå Evite quando**:
- Apenas uma transforma√ß√£o simples (use Dataflow ou Notebook direto)
- N√£o precisa de orquestra√ß√£o

**Exemplo de Uso**:
```
Cen√°rio: Pipeline de ETL completo
1. Copiar dados de SQL Server para Lakehouse
2. Executar notebook para transforma√ß√£o
3. Executar stored procedure para carregar DW
4. Enviar notifica√ß√£o de sucesso/falha
Ferramenta: Pipeline
Raz√£o: M√∫ltiplas atividades com depend√™ncias
```

#### Notebook

**‚úÖ Use quando**:
- Transforma√ß√µes complexas que requerem c√≥digo
- Dados muito grandes (TB+)
- Precisa de controle fino sobre Spark
- Machine Learning / Data Science
- L√≥gica de neg√≥cio customizada
- Precisa de otimiza√ß√µes espec√≠ficas (partitioning, caching)

**‚ùå Evite quando**:
- Transforma√ß√µes simples que podem ser feitas no Dataflow
- Equipe n√£o tem experi√™ncia com Spark/Python

**Exemplo de Uso**:
```
Cen√°rio: Implementar SCD Tipo 2 em dimens√£o de clientes
Ferramenta: Notebook (PySpark)
Raz√£o: L√≥gica complexa de merge, controle fino sobre Delta Lake
```

### 1.3 Padr√£o Combinado (Recomendado)

**Melhor Pr√°tica**: Combinar as tr√™s ferramentas

```mermaid
graph LR
    A[Pipeline] --> B[Copy Activity:<br/>SQL ‚Üí Lakehouse]
    A --> C[Dataflow Gen2:<br/>Limpeza Inicial]
    A --> D[Notebook:<br/>Transforma√ß√£o Complexa]
    A --> E[Stored Procedure:<br/>Load DW]
    
    B --> C
    C --> D
    D --> E
    
    style A fill:#e3f2fd
    style C fill:#e8f5e9
    style D fill:#f3e5f5
```

---

## 2. Pipelines - Componentes e Atividades

### 2.1 Anatomia de um Pipeline

```
Pipeline: Daily_Sales_ETL
‚îú‚îÄ‚îÄ Parameters
‚îÇ   ‚îú‚îÄ‚îÄ ProcessDate (String)
‚îÇ   ‚îú‚îÄ‚îÄ SourcePath (String)
‚îÇ   ‚îî‚îÄ‚îÄ Environment (String)
‚îú‚îÄ‚îÄ Variables
‚îÇ   ‚îú‚îÄ‚îÄ RowCount (Integer)
‚îÇ   ‚îî‚îÄ‚îÄ ErrorMessage (String)
‚îî‚îÄ‚îÄ Activities
    ‚îú‚îÄ‚îÄ Copy Data (Source ‚Üí Lakehouse)
    ‚îú‚îÄ‚îÄ Notebook (Transform Data)
    ‚îú‚îÄ‚îÄ If Condition (Check Row Count)
    ‚îú‚îÄ‚îÄ Stored Procedure (Load DW)
    ‚îî‚îÄ‚îÄ Web (Send Notification)
```

### 2.2 Atividades de Pipeline

#### Atividades de Movimenta√ß√£o de Dados

| Atividade | Descri√ß√£o | Quando Usar |
|-----------|-----------|-------------|
| **Copy Data** | Copia dados de origem para destino | Ingest√£o de dados de fontes externas |
| **Delete** | Deleta arquivos/pastas | Limpeza de dados tempor√°rios |

#### Atividades de Transforma√ß√£o

| Atividade | Descri√ß√£o | Quando Usar |
|-----------|-----------|-------------|
| **Dataflow** | Executa Dataflow Gen2 | Transforma√ß√µes low-code |
| **Notebook** | Executa notebook Spark | Transforma√ß√µes code-first |
| **Stored Procedure** | Executa SP no Warehouse | L√≥gica SQL no DW |

#### Atividades de Controle de Fluxo

| Atividade | Descri√ß√£o | Quando Usar |
|-----------|-----------|-------------|
| **If Condition** | Execu√ß√£o condicional | L√≥gica if-then-else |
| **Switch** | M√∫ltiplas condi√ß√µes | L√≥gica switch-case |
| **For Each** | Loop sobre array | Processar m√∫ltiplos arquivos |
| **Until** | Loop at√© condi√ß√£o | Retry at√© sucesso |
| **Wait** | Pausa execu√ß√£o | Aguardar processo externo |
| **Set Variable** | Define valor de vari√°vel | Armazenar valores intermedi√°rios |
| **Append Variable** | Adiciona a array | Coletar resultados |

#### Atividades de Integra√ß√£o

| Atividade | Descri√ß√£o | Quando Usar |
|-----------|-----------|-------------|
| **Web** | Chama API REST | Notifica√ß√µes, integra√ß√µes |
| **Webhook** | Chama webhook | Integra√ß√£o com sistemas externos |
| **Script** | Executa script (Python, PowerShell) | Automa√ß√µes customizadas |

### 2.3 Exemplo de Pipeline Completo

```json
{
  "name": "Daily_Sales_ETL_Pipeline",
  "properties": {
    "activities": [
      {
        "name": "Copy_Sales_Data",
        "type": "Copy",
        "inputs": [
          {
            "referenceName": "SQL_Server_Sales",
            "type": "DatasetReference"
          }
        ],
        "outputs": [
          {
            "referenceName": "Lakehouse_Bronze_Sales",
            "type": "DatasetReference"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "SqlServerSource",
            "sqlReaderQuery": "SELECT * FROM Sales WHERE SaleDate = '@{pipeline().parameters.ProcessDate}'"
          },
          "sink": {
            "type": "LakehouseSink",
            "writeBehavior": "Append"
          }
        }
      },
      {
        "name": "Transform_Sales_Notebook",
        "type": "Notebook",
        "dependsOn": [
          {
            "activity": "Copy_Sales_Data",
            "dependencyConditions": ["Succeeded"]
          }
        ],
        "typeProperties": {
          "notebookPath": "/Notebooks/Transform_Sales",
          "parameters": {
            "process_date": {
              "value": "@pipeline().parameters.ProcessDate",
              "type": "String"
            }
          }
        }
      },
      {
        "name": "Load_DW_StoredProc",
        "type": "SqlServerStoredProcedure",
        "dependsOn": [
          {
            "activity": "Transform_Sales_Notebook",
            "dependencyConditions": ["Succeeded"]
          }
        ],
        "typeProperties": {
          "storedProcedureName": "sp_Load_Fact_Sales",
          "storedProcedureParameters": {
            "ProcessDate": {
              "value": "@pipeline().parameters.ProcessDate",
              "type": "String"
            }
          }
        }
      },
      {
        "name": "Send_Success_Notification",
        "type": "Web",
        "dependsOn": [
          {
            "activity": "Load_DW_StoredProc",
            "dependencyConditions": ["Succeeded"]
          }
        ],
        "typeProperties": {
          "url": "https://hooks.slack.com/services/YOUR/WEBHOOK/URL",
          "method": "POST",
          "body": {
            "text": "Pipeline Daily_Sales_ETL completed successfully for @{pipeline().parameters.ProcessDate}"
          }
        }
      },
      {
        "name": "Send_Failure_Notification",
        "type": "Web",
        "dependsOn": [
          {
            "activity": "Load_DW_StoredProc",
            "dependencyConditions": ["Failed"]
          }
        ],
        "typeProperties": {
          "url": "https://hooks.slack.com/services/YOUR/WEBHOOK/URL",
          "method": "POST",
          "body": {
            "text": "‚ùå Pipeline Daily_Sales_ETL FAILED for @{pipeline().parameters.ProcessDate}. Error: @{activity('Load_DW_StoredProc').error.message}"
          }
        }
      }
    ],
    "parameters": {
      "ProcessDate": {
        "type": "String",
        "defaultValue": "@formatDateTime(utcnow(), 'yyyy-MM-dd')"
      }
    }
  }
}
```

---

## 3. Par√¢metros e Vari√°veis

### 3.1 Par√¢metros de Pipeline

**Defini√ß√£o**: Valores passados para o pipeline em tempo de execu√ß√£o.

**Quando usar**: Tornar pipelines reutiliz√°veis.

**Tipos Suportados**:
- String
- Int
- Float
- Bool
- Array
- Object

**Exemplo de Defini√ß√£o**:
```json
{
  "parameters": {
    "SourcePath": {
      "type": "String",
      "defaultValue": "/bronze/sales/"
    },
    "ProcessDate": {
      "type": "String",
      "defaultValue": "@formatDateTime(utcnow(), 'yyyy-MM-dd')"
    },
    "BatchSize": {
      "type": "Int",
      "defaultValue": 1000
    },
    "EnableLogging": {
      "type": "Bool",
      "defaultValue": true
    },
    "Regions": {
      "type": "Array",
      "defaultValue": ["North", "South", "East", "West"]
    }
  }
}
```

**Uso em Atividades**:
```
@pipeline().parameters.SourcePath
@pipeline().parameters.ProcessDate
@pipeline().parameters.BatchSize
```

### 3.2 Vari√°veis de Pipeline

**Defini√ß√£o**: Valores armazenados e modificados durante a execu√ß√£o do pipeline.

**Quando usar**: Armazenar valores intermedi√°rios, contadores, flags.

**Diferen√ßa de Par√¢metros**:
- Par√¢metros: Somente leitura, definidos antes da execu√ß√£o
- Vari√°veis: Leitura/escrita, modificadas durante a execu√ß√£o

**Exemplo**:
```json
{
  "variables": {
    "RowCount": {
      "type": "Integer",
      "defaultValue": 0
    },
    "ErrorMessage": {
      "type": "String",
      "defaultValue": ""
    },
    "ProcessedFiles": {
      "type": "Array",
      "defaultValue": []
    }
  }
}
```

**Uso**:
```
// Ler vari√°vel
@variables('RowCount')

// Definir vari√°vel (atividade Set Variable)
@activity('Copy_Data').output.rowsCopied

// Adicionar a array (atividade Append Variable)
@concat(variables('ProcessedFiles'), item().name)
```

---

## 4. Express√µes Din√¢micas e Fun√ß√µes do Sistema

### 4.1 Fun√ß√µes de Data/Hora

| Fun√ß√£o | Descri√ß√£o | Exemplo | Resultado |
|--------|-----------|---------|-----------|
| `utcnow()` | Data/hora atual UTC | `@utcnow()` | `2024-01-15T10:30:00Z` |
| `adddays(date, days)` | Adiciona dias | `@adddays(utcnow(), -1)` | Ontem |
| `addhours(date, hours)` | Adiciona horas | `@addhours(utcnow(), -3)` | 3 horas atr√°s |
| `formatDateTime(date, format)` | Formata data | `@formatDateTime(utcnow(), 'yyyy-MM-dd')` | `2024-01-15` |
| `dayOfWeek(date)` | Dia da semana (0-6) | `@dayOfWeek(utcnow())` | `1` (Monday) |
| `dayOfMonth(date)` | Dia do m√™s | `@dayOfMonth(utcnow())` | `15` |

**Exemplos Pr√°ticos**:

```javascript
// Processar dados de ontem
@formatDateTime(adddays(utcnow(), -1), 'yyyy-MM-dd')

// Nome de arquivo com timestamp
@concat('sales_', formatDateTime(utcnow(), 'yyyyMMdd_HHmmss'), '.csv')

// Primeiro dia do m√™s atual
@formatDateTime(startOfMonth(utcnow()), 'yyyy-MM-dd')

// √öltimo dia do m√™s anterior
@formatDateTime(adddays(startOfMonth(utcnow()), -1), 'yyyy-MM-dd')
```

### 4.2 Fun√ß√µes de String

| Fun√ß√£o | Descri√ß√£o | Exemplo |
|--------|-----------|---------|
| `concat(str1, str2, ...)` | Concatena strings | `@concat('prefix_', parameters('name'))` |
| `substring(str, start, length)` | Extrai substring | `@substring('hello', 0, 2)` ‚Üí `'he'` |
| `replace(str, old, new)` | Substitui texto | `@replace('hello', 'l', 'L')` ‚Üí `'heLLo'` |
| `toLower(str)` | Min√∫sculas | `@toLower('HELLO')` ‚Üí `'hello'` |
| `toUpper(str)` | Mai√∫sculas | `@toUpper('hello')` ‚Üí `'HELLO'` |
| `trim(str)` | Remove espa√ßos | `@trim(' hello ')` ‚Üí `'hello'` |
| `split(str, delimiter)` | Divide string | `@split('a,b,c', ',')` ‚Üí `['a','b','c']` |

### 4.3 Fun√ß√µes L√≥gicas

| Fun√ß√£o | Descri√ß√£o | Exemplo |
|--------|-----------|---------|
| `if(condition, true_value, false_value)` | Condicional | `@if(equals(variables('status'), 'success'), 'OK', 'ERROR')` |
| `and(cond1, cond2)` | E l√≥gico | `@and(greater(variables('count'), 0), less(variables('count'), 100))` |
| `or(cond1, cond2)` | OU l√≥gico | `@or(equals(variables('env'), 'DEV'), equals(variables('env'), 'TEST'))` |
| `not(condition)` | Nega√ß√£o | `@not(equals(variables('status'), 'failed'))` |
| `equals(val1, val2)` | Igualdade | `@equals(parameters('env'), 'PROD')` |
| `greater(val1, val2)` | Maior que | `@greater(variables('count'), 1000)` |
| `less(val1, val2)` | Menor que | `@less(variables('count'), 100)` |

### 4.4 Fun√ß√µes de Pipeline

| Fun√ß√£o | Descri√ß√£o | Exemplo |
|--------|-----------|---------|
| `pipeline().RunId` | ID da execu√ß√£o | `@pipeline().RunId` |
| `pipeline().Pipeline` | Nome do pipeline | `@pipeline().Pipeline` |
| `pipeline().parameters.ParamName` | Valor do par√¢metro | `@pipeline().parameters.ProcessDate` |
| `pipeline().TriggerType` | Tipo de trigger | `@pipeline().TriggerType` |
| `pipeline().TriggerName` | Nome do trigger | `@pipeline().TriggerName` |
| `pipeline().TriggerTime` | Hora do trigger | `@pipeline().TriggerTime` |

### 4.5 Fun√ß√µes de Atividade

| Fun√ß√£o | Descri√ß√£o | Exemplo |
|--------|-----------|---------|
| `activity('ActivityName').output` | Output da atividade | `@activity('Copy_Data').output.rowsCopied` |
| `activity('ActivityName').error` | Erro da atividade | `@activity('Transform').error.message` |
| `activity('ActivityName').status` | Status da atividade | `@activity('Copy_Data').status` |

**Exemplo Completo**:
```javascript
// Construir caminho din√¢mico com data
@concat(
  parameters('BasePath'),
  '/year=', formatDateTime(utcnow(), 'yyyy'),
  '/month=', formatDateTime(utcnow(), 'MM'),
  '/day=', formatDateTime(utcnow(), 'dd'),
  '/sales_', formatDateTime(utcnow(), 'yyyyMMdd_HHmmss'), '.parquet'
)

// Resultado: /bronze/sales/year=2024/month=01/day=15/sales_20240115_103045.parquet
```

---

## 5. Agendamento e Gatilhos

### 5.1 Tipos de Triggers

| Tipo | Quando Usar | Configura√ß√£o |
|------|-------------|--------------|
| **Schedule Trigger** | Execu√ß√£o em hor√°rios fixos | Frequ√™ncia, hora, timezone |
| **Tumbling Window Trigger** | Execu√ß√£o em janelas de tempo fixas com depend√™ncias | Frequ√™ncia, janela, delay |
| **Event Trigger** | Execu√ß√£o quando evento ocorre | Evento OneLake (blob created/deleted) |
| **Manual Trigger** | Execu√ß√£o sob demanda | Bot√£o "Run now" |

### 5.2 Schedule Trigger - Exemplos

#### Exemplo 1: Execu√ß√£o Di√°ria

```json
{
  "name": "Daily_2AM_Trigger",
  "properties": {
    "type": "ScheduleTrigger",
    "typeProperties": {
      "recurrence": {
        "frequency": "Day",
        "interval": 1,
        "startTime": "2024-01-01T02:00:00Z",
        "timeZone": "E. South America Standard Time"
      }
    },
    "pipelines": [
      {
        "pipelineReference": {
          "referenceName": "Daily_Sales_ETL",
          "type": "PipelineReference"
        },
        "parameters": {
          "ProcessDate": "@formatDateTime(adddays(utcnow(), -1), 'yyyy-MM-dd')"
        }
      }
    ]
  }
}
```

#### Exemplo 2: Execu√ß√£o a Cada Hora (Hor√°rio Comercial)

```json
{
  "name": "Hourly_Business_Hours_Trigger",
  "properties": {
    "type": "ScheduleTrigger",
    "typeProperties": {
      "recurrence": {
        "frequency": "Hour",
        "interval": 1,
        "startTime": "2024-01-01T08:00:00Z",
        "endTime": "2024-01-01T18:00:00Z",
        "timeZone": "E. South America Standard Time",
        "schedule": {
          "weekDays": ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]
        }
      }
    }
  }
}
```

#### Exemplo 3: Execu√ß√£o Semanal

```json
{
  "name": "Weekly_Monday_Trigger",
  "properties": {
    "type": "ScheduleTrigger",
    "typeProperties": {
      "recurrence": {
        "frequency": "Week",
        "interval": 1,
        "startTime": "2024-01-01T06:00:00Z",
        "timeZone": "E. South America Standard Time",
        "schedule": {
          "weekDays": ["Monday"]
        }
      }
    }
  }
}
```

### 5.3 Event Trigger - OneLake

**Cen√°rio**: Processar automaticamente quando novo arquivo chegar

```json
{
  "name": "OneLake_File_Arrival_Trigger",
  "properties": {
    "type": "BlobEventsTrigger",
    "typeProperties": {
      "blobPathBeginsWith": "/lakehouse/Files/bronze/incoming/",
      "blobPathEndsWith": ".csv",
      "events": ["Microsoft.Storage.BlobCreated"],
      "scope": "/workspaces/{workspace-id}/lakehouses/{lakehouse-id}"
    },
    "pipelines": [
      {
        "pipelineReference": {
          "referenceName": "Process_Incoming_File",
          "type": "PipelineReference"
        },
        "parameters": {
          "FileName": "@triggerBody().fileName",
          "FilePath": "@triggerBody().folderPath"
        }
      }
    ]
  }
}
```

**Quando um arquivo `sales_2024-01-15.csv` √© carregado em `/Files/bronze/incoming/`, o pipeline √© automaticamente acionado.**

### 5.4 Tumbling Window Trigger

**Quando usar**: Processar dados em janelas de tempo fixas com depend√™ncias entre execu√ß√µes.

```json
{
  "name": "Hourly_Tumbling_Window",
  "properties": {
    "type": "TumblingWindowTrigger",
    "typeProperties": {
      "frequency": "Hour",
      "interval": 1,
      "startTime": "2024-01-01T00:00:00Z",
      "delay": "00:15:00",
      "maxConcurrency": 1,
      "retryPolicy": {
        "count": 3,
        "intervalInSeconds": 300
      }
    },
    "pipeline": {
      "pipelineReference": {
        "referenceName": "Hourly_Aggregation",
        "type": "PipelineReference"
      },
      "parameters": {
        "WindowStart": "@trigger().outputs.windowStartTime",
        "WindowEnd": "@trigger().outputs.windowEndTime"
      }
    }
  }
}
```

---

## 6. Dependency Patterns (Padr√µes de Depend√™ncia)

### 6.1 Tipos de Depend√™ncia

| Tipo | Quando Executa | Uso |
|------|----------------|-----|
| **Succeeded** | Atividade anterior teve sucesso | Fluxo normal (happy path) |
| **Failed** | Atividade anterior falhou | Error handling |
| **Skipped** | Atividade anterior foi pulada | L√≥gica condicional |
| **Completed** | Atividade anterior completou (sucesso ou falha) | Cleanup, logging |

### 6.2 Padr√£o: Success Path

```mermaid
graph LR
    A[Copy Data] -->|Succeeded| B[Transform]
    B -->|Succeeded| C[Load DW]
    C -->|Succeeded| D[Send Success Email]
    
    style A fill:#e8f5e9
    style B fill:#e8f5e9
    style C fill:#e8f5e9
    style D fill:#e8f5e9
```

### 6.3 Padr√£o: Error Handling

```mermaid
graph TD
    A[Copy Data] -->|Succeeded| B[Transform]
    A -->|Failed| E[Log Error]
    B -->|Succeeded| C[Load DW]
    B -->|Failed| E
    C -->|Succeeded| D[Send Success Email]
    C -->|Failed| E
    E --> F[Send Failure Email]
    
    style A fill:#e8f5e9
    style B fill:#e8f5e9
    style C fill:#e8f5e9
    style D fill:#e8f5e9
    style E fill:#ffebee
    style F fill:#ffebee
```

### 6.4 Padr√£o: Cleanup (Always Execute)

```mermaid
graph TD
    A[Copy Data] -->|Succeeded| B[Transform]
    A -->|Completed| C[Delete Temp Files]
    B -->|Completed| C
    
    style A fill:#e8f5e9
    style B fill:#e8f5e9
    style C fill:#fff4e1
```

### 6.5 Exemplo de C√≥digo

```json
{
  "name": "Transform_Data",
  "type": "Notebook",
  "dependsOn": [
    {
      "activity": "Copy_Data",
      "dependencyConditions": ["Succeeded"]
    }
  ]
},
{
  "name": "Log_Error",
  "type": "Web",
  "dependsOn": [
    {
      "activity": "Transform_Data",
      "dependencyConditions": ["Failed"]
    }
  ]
},
{
  "name": "Cleanup_Temp_Files",
  "type": "Delete",
  "dependsOn": [
    {
      "activity": "Transform_Data",
      "dependencyConditions": ["Completed"]
    }
  ]
}
```

---

## 7. Error Handling e Retry Logic

### 7.1 Retry Policy

**Configura√ß√£o em Atividade**:

```json
{
  "name": "Copy_Data_With_Retry",
  "type": "Copy",
  "policy": {
    "retry": 3,
    "retryIntervalInSeconds": 300,
    "secureOutput": false,
    "secureInput": false,
    "timeout": "01:00:00"
  }
}
```

**Par√¢metros**:
- `retry`: N√∫mero de tentativas (0-3)
- `retryIntervalInSeconds`: Intervalo entre tentativas
- `timeout`: Tempo m√°ximo de execu√ß√£o

### 7.2 Padr√£o: Try-Catch

```mermaid
graph TD
    A[Try: Main Logic] -->|Succeeded| B[Success Handler]
    A -->|Failed| C[Catch: Error Handler]
    C --> D[Log Error]
    C --> E[Send Alert]
    C --> F[Rollback]
    
    style A fill:#e8f5e9
    style B fill:#e8f5e9
    style C fill:#ffebee
```

### 7.3 Padr√£o: Until Loop com Retry

```json
{
  "name": "Retry_Until_Success",
  "type": "Until",
  "typeProperties": {
    "expression": {
      "value": "@equals(variables('Status'), 'Success')",
      "type": "Expression"
    },
    "activities": [
      {
        "name": "Try_Operation",
        "type": "Copy"
      },
      {
        "name": "Set_Status_Success",
        "type": "SetVariable",
        "dependsOn": [
          {
            "activity": "Try_Operation",
            "dependencyConditions": ["Succeeded"]
          }
        ],
        "typeProperties": {
          "variableName": "Status",
          "value": "Success"
        }
      },
      {
        "name": "Wait_Before_Retry",
        "type": "Wait",
        "dependsOn": [
          {
            "activity": "Try_Operation",
            "dependencyConditions": ["Failed"]
          }
        ],
        "typeProperties": {
          "waitTimeInSeconds": 60
        }
      }
    ],
    "timeout": "01:00:00"
  }
}
```

---

## 8. Padr√µes Avan√ßados de Orquestra√ß√£o

### 8.1 Padr√£o: For Each (Processar M√∫ltiplos Arquivos)

```json
{
  "name": "Process_All_Files",
  "type": "ForEach",
  "typeProperties": {
    "items": {
      "value": "@pipeline().parameters.FileList",
      "type": "Expression"
    },
    "isSequential": false,
    "batchCount": 4,
    "activities": [
      {
        "name": "Process_File",
        "type": "Notebook",
        "typeProperties": {
          "notebookPath": "/Notebooks/Process_File",
          "parameters": {
            "file_path": {
              "value": "@item().path",
              "type": "String"
            }
          }
        }
      }
    ]
  }
}
```

**Par√¢metros**:
- `isSequential`: false = paralelo, true = sequencial
- `batchCount`: N√∫mero de itera√ß√µes paralelas (m√°x: 50)

### 8.2 Padr√£o: Switch (M√∫ltiplas Condi√ß√µes)

```json
{
  "name": "Route_By_FileType",
  "type": "Switch",
  "typeProperties": {
    "on": {
      "value": "@pipeline().parameters.FileType",
      "type": "Expression"
    },
    "cases": [
      {
        "value": "CSV",
        "activities": [
          {
            "name": "Process_CSV",
            "type": "Dataflow"
          }
        ]
      },
      {
        "value": "JSON",
        "activities": [
          {
            "name": "Process_JSON",
            "type": "Notebook"
          }
        ]
      },
      {
        "value": "Parquet",
        "activities": [
          {
            "name": "Process_Parquet",
            "type": "Copy"
          }
        ]
      }
    ],
    "defaultActivities": [
      {
        "name": "Unsupported_FileType_Error",
        "type": "Web"
      }
    ]
  }
}
```

### 8.3 Padr√£o: Incremental Load com Watermark

```mermaid
graph TD
    A[Get Last Watermark] --> B[Copy New Data<br/>WHERE ModifiedDate > Watermark]
    B --> C[Update Watermark<br/>To Max ModifiedDate]
    
    style A fill:#fff4e1
    style B fill:#e8f5e9
    style C fill:#e3f2fd
```

**Implementa√ß√£o**:

```json
{
  "activities": [
    {
      "name": "Get_Last_Watermark",
      "type": "Lookup",
      "typeProperties": {
        "source": {
          "type": "WarehouseSource",
          "sqlReaderQuery": "SELECT MAX(LastModifiedDate) AS Watermark FROM control.watermark WHERE TableName = 'Sales'"
        }
      }
    },
    {
      "name": "Copy_Incremental_Data",
      "type": "Copy",
      "dependsOn": [
        {
          "activity": "Get_Last_Watermark",
          "dependencyConditions": ["Succeeded"]
        }
      ],
      "typeProperties": {
        "source": {
          "type": "SqlServerSource",
          "sqlReaderQuery": "SELECT * FROM Sales WHERE ModifiedDate > '@{activity('Get_Last_Watermark').output.firstRow.Watermark}'"
        }
      }
    },
    {
      "name": "Update_Watermark",
      "type": "SqlServerStoredProcedure",
      "dependsOn": [
        {
          "activity": "Copy_Incremental_Data",
          "dependencyConditions": ["Succeeded"]
        }
      ],
      "typeProperties": {
        "storedProcedureName": "sp_Update_Watermark",
        "storedProcedureParameters": {
          "TableName": "Sales",
          "NewWatermark": "@{activity('Copy_Incremental_Data').output.executionDetails[0].source.maxValue}"
        }
      }
    }
  ]
}
```

---

## 9. Best Practices - Orquestra√ß√£o

### ‚úÖ Design

1. **Modularidade**:
   - Crie pipelines reutiliz√°veis
   - Use par√¢metros extensivamente
   - Separe l√≥gica de neg√≥cio em notebooks

2. **Naming Conventions**:
   ```
   Pipeline: [Frequency]_[Entity]_[Action]
   Exemplo: Daily_Sales_ETL
   
   Atividade: [Action]_[Entity]
   Exemplo: Copy_Sales_Data
   ```

3. **Organiza√ß√£o**:
   - Agrupe atividades relacionadas visualmente
   - Use anota√ß√µes para documentar l√≥gica complexa
   - Mantenha pipelines com < 20 atividades

### ‚úÖ Performance

1. **Paraleliza√ß√£o**:
   - Use ForEach com `isSequential: false`
   - Configure `batchCount` apropriado
   - Evite depend√™ncias desnecess√°rias

2. **Otimiza√ß√£o de Copy**:
   - Use particionamento para grandes volumes
   - Configure `parallelCopies` apropriado
   - Use compression quando poss√≠vel

3. **Timeout**:
   - Configure timeouts realistas
   - Evite timeouts muito longos (> 24h)

### ‚úÖ Confiabilidade

1. **Error Handling**:
   - Sempre implemente error handling
   - Use retry policy para opera√ß√µes transientes
   - Log erros detalhados

2. **Idempot√™ncia**:
   - Pipelines devem ser idempotentes
   - Suportar re-execu√ß√£o sem efeitos colaterais
   - Use upsert em vez de insert

3. **Monitoramento**:
   - Configure alertas para falhas
   - Monitore dura√ß√£o de execu√ß√£o
   - Track SLA compliance

---

## Refer√™ncias e Recursos

### Documenta√ß√£o Oficial

1. [Data Pipelines Overview](https://learn.microsoft.com/fabric/data-factory/data-pipelines-overview)
2. [Pipeline Activities](https://learn.microsoft.com/fabric/data-factory/activity-overview)
3. [Pipeline Triggers](https://learn.microsoft.com/fabric/data-factory/pipeline-triggers)
4. [Expression Language](https://learn.microsoft.com/azure/data-factory/control-flow-expression-language-functions)
5. [Dataflow Gen2](https://learn.microsoft.com/fabric/data-factory/dataflows-gen2-overview)
6. [Guia de Estudo DP-700](https://learn.microsoft.com/credentials/certifications/resources/study-guides/dp-700)

---

## Pontos-Chave para o Exame DP-700

üéØ **Memorize**:
- **Dataflow Gen2**: Low-code, Power Query M, bom para transforma√ß√µes simples
- **Pipeline**: Orquestra√ß√£o, coordena m√∫ltiplas atividades
- **Notebook**: Code-first, Spark, transforma√ß√µes complexas
- **Dependency Conditions**: Succeeded, Failed, Skipped, Completed
- **Triggers**: Schedule, Tumbling Window, Event, Manual
- **Par√¢metros**: Somente leitura, definidos antes da execu√ß√£o
- **Vari√°veis**: Leitura/escrita, modificadas durante execu√ß√£o
- **ForEach**: `isSequential: false` para paralelo, `batchCount` para controlar paralelismo
- **Retry Policy**: M√°ximo 3 tentativas

üéØ **Entenda**:
- Quando usar Dataflow vs Pipeline vs Notebook
- Como construir express√µes din√¢micas
- Diferen√ßa entre par√¢metros e vari√°veis
- Como implementar error handling
- Padr√µes de orquestra√ß√£o (incremental load, for each, switch)
- Como configurar triggers (schedule vs event)
- Dependency patterns e quando usar cada um

