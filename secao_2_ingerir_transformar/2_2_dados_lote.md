# 2.2 Ingerir e transformar dados em lote

A ingest√£o e transforma√ß√£o de dados em lote (batch) √© o processo mais comum na engenharia de dados, onde grandes volumes de dados s√£o processados em intervalos programados. O Microsoft Fabric oferece um conjunto rico e integrado de ferramentas para executar essas tarefas de forma **eficiente, escal√°vel e otimizada**, desde a escolha do armazenamento at√© a transforma√ß√£o final dos dados.

## Vis√£o Geral do Batch Processing

```mermaid
graph LR
    A[Fontes de Dados] --> B[Ingest√£o]
    B --> C[Lakehouse Bronze]
    C --> D[Transforma√ß√£o Spark]
    D --> E[Lakehouse Silver]
    E --> F[Transforma√ß√£o Avan√ßada]
    F --> G[Warehouse Gold]
    
    style A fill:#e8f5e9
    style C fill:#fff4e1
    style E fill:#e3f2fd
    style G fill:#f3e5f5
```

---

## 1. Escolhendo Armazenamento e Ferramentas

### 1.1 Compara√ß√£o de Armazenamento

| Aspecto | Lakehouse | Data Warehouse |
|---------|-----------|----------------|
| **Formato** | Delta Lake (Parquet + log) | Proprietary columnar |
| **Acesso** | Spark, SQL Endpoint, Power BI | T-SQL, Power BI |
| **Melhor para** | Dados brutos, semi-estruturados, ML | Dados curados, modelos dimensionais |
| **Performance** | Excelente para scan completo | Excelente para queries pontuais |
| **Custo** | Menor (armazenamento OneLake) | Maior (compute dedicado) |
| **Escalabilidade** | Horizontal (Spark) | Vertical + Horizontal |
| **Schema** | Schema-on-read | Schema-on-write |
| **Camadas** | Bronze, Silver | Gold |

### 1.2 Ferramentas de Transforma√ß√£o

| Ferramenta | Quando Usar | Vantagens | Desvantagens |
|------------|-------------|-----------|--------------|
| **Dataflow Gen2** | Transforma√ß√µes simples, usu√°rios de Power BI | Low-code, familiar | Limitado para grandes volumes |
| **Notebooks (PySpark)** | Transforma√ß√µes complexas, grandes volumes | Flex√≠vel, escal√°vel | Requer conhecimento de Spark |
| **T-SQL (Warehouse)** | Transforma√ß√µes no DW, usu√°rios SQL | Familiar, otimizado | Apenas no Warehouse |
| **KQL** | An√°lise explorat√≥ria, s√©ries temporais | R√°pido para an√°lise ad-hoc | Menos comum para ETL |

---

## 2. Otimiza√ß√µes Spark

### 2.1 Particionamento

**Conceito**: Dividir dados em parti√ß√µes para processamento paralelo.

#### Particionamento por Coluna

```python
# Particionar por data ao escrever
df_sales.write \
    .format("delta") \
    .partitionBy("sale_date") \
    .mode("append") \
    .saveAsTable("lakehouse.sales_partitioned")

# Resultado: Estrutura de pastas
# /sales_partitioned/
#   sale_date=2024-01-01/
#   sale_date=2024-01-02/
#   sale_date=2024-01-03/
```

**Quando usar**:
- ‚úÖ Coluna com cardinalidade m√©dia (100-10.000 valores √∫nicos)
- ‚úÖ Queries frequentemente filtram por essa coluna
- ‚ùå Evite alta cardinalidade (ex: customer_id com milh√µes de valores)
- ‚ùå Evite baixa cardinalidade (ex: status com 3 valores)

#### Reparticionamento em Mem√≥ria

```python
# Reparticionar para otimizar processamento
df_large = spark.read.table("lakehouse.large_table")

# Aumentar parti√ß√µes para paralelismo
df_repartitioned = df_large.repartition(200)

# Particionar por coluna espec√≠fica
df_by_region = df_large.repartition("region")

# Coalesce (reduzir parti√ß√µes - mais eficiente que repartition)
df_small = df_large.coalesce(10)
```

**Regra Geral**:
- Parti√ß√µes ideais: 2-4x n√∫mero de cores
- Tamanho ideal por parti√ß√£o: 128MB - 1GB

### 2.2 Caching e Persistence

```python
from pyspark import StorageLevel

# Cache em mem√≥ria (padr√£o)
df_cached = df.cache()

# Persist com n√≠vel de armazenamento espec√≠fico
df.persist(StorageLevel.MEMORY_AND_DISK)

# N√≠veis dispon√≠veis:
# - MEMORY_ONLY: Apenas mem√≥ria
# - MEMORY_AND_DISK: Mem√≥ria + disco se n√£o couber
# - DISK_ONLY: Apenas disco
# - MEMORY_ONLY_SER: Mem√≥ria serializada (menos espa√ßo)
# - MEMORY_AND_DISK_SER: Mem√≥ria serializada + disco

# Liberar cache quando n√£o precisar mais
df.unpersist()
```

**Quando usar cache**:
- ‚úÖ DataFrame usado m√∫ltiplas vezes
- ‚úÖ Transforma√ß√µes custosas antes do cache
- ‚ùå DataFrame usado apenas uma vez
- ‚ùå Dados muito grandes (n√£o cabem em mem√≥ria)

### 2.3 Broadcast Joins

**Problema**: Joins de tabelas grandes s√£o custosos (shuffle de dados).

**Solu√ß√£o**: Broadcast da tabela pequena para todos os workers.

```python
from pyspark.sql.functions import broadcast

# Tabela grande: fact_sales (100M registros)
df_sales = spark.read.table("lakehouse.fact_sales")

# Tabela pequena: dim_product (10K registros)
df_product = spark.read.table("lakehouse.dim_product")

# Join normal (shuffle - lento)
df_joined_slow = df_sales.join(df_product, "product_id")

# Broadcast join (r√°pido)
df_joined_fast = df_sales.join(
    broadcast(df_product), 
    "product_id"
)
```

**Limite**: Tabela broadcast deve ser < 10GB (configur√°vel).

**Auto-broadcast**: Spark automaticamente faz broadcast se tabela < 10MB.

### 2.4 Predicate Pushdown

**Conceito**: Filtrar dados na fonte antes de carregar.

```python
# Ruim: Carrega tudo, depois filtra
df_all = spark.read.table("lakehouse.sales")
df_filtered = df_all.filter(col("sale_date") == "2024-01-15")

# Bom: Filtra na leitura (predicate pushdown)
df_filtered = spark.read.table("lakehouse.sales") \
    .filter(col("sale_date") == "2024-01-15")

# Melhor: Se tabela particionada por sale_date
# Spark l√™ apenas a parti√ß√£o relevante
```

### 2.5 Column Pruning

**Conceito**: Ler apenas colunas necess√°rias.

```python
# Ruim: L√™ todas as colunas
df_all = spark.read.table("lakehouse.sales")
df_result = df_all.select("product_id", "amount")

# Bom: L√™ apenas colunas necess√°rias
df_result = spark.read.table("lakehouse.sales") \
    .select("product_id", "amount")
```

**Benef√≠cio**: Reduz I/O significativamente (formato colunar).

---

## 3. Atalhos (Shortcuts)

### 3.1 Tipos de Shortcuts

| Tipo | Fonte | Quando Usar |
|------|-------|-------------|
| **OneLake Shortcut** | Outro Lakehouse no Fabric | Compartilhar dados entre workspaces |
| **ADLS Gen2 Shortcut** | Azure Data Lake Storage | Acessar dados externos sem c√≥pia |
| **S3 Shortcut** | Amazon S3 | Acessar dados na AWS |

### 3.2 Criando Shortcuts

**Via UI**:
1. Lakehouse ‚Üí Files ‚Üí New shortcut
2. Selecionar tipo (OneLake, ADLS, S3)
3. Configurar conex√£o
4. Especificar caminho

**Via API (exemplo)**:
```python
# Criar shortcut via Fabric REST API
import requests

url = "https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}/shortcuts"

payload = {
    "name": "external_data",
    "path": "/Files/external",
    "target": {
        "type": "AdlsGen2",
        "connectionId": "{connection_id}",
        "location": "https://storageaccount.dfs.core.windows.net/container/path"
    }
}

response = requests.post(url, json=payload, headers=headers)
```

### 3.3 Shortcuts vs C√≥pia de Dados

| Aspecto | Shortcut | C√≥pia |
|---------|----------|-------|
| **Lat√™ncia** | Instant√¢nea | Depende do volume |
| **Armazenamento** | N√£o duplica | Duplica dados |
| **Custo** | Menor | Maior |
| **Performance** | Pode ser mais lenta (rede) | Mais r√°pida (local) |
| **Atualiza√ß√£o** | Sempre atual | Precisa re-copiar |
| **Quando usar** | Dados que mudam frequentemente | Dados est√°ticos, performance cr√≠tica |

---

## 4. Espelhamento (Mirroring)

### 4.1 Fontes Suportadas

| Fonte | Lat√™ncia | Quando Usar |
|-------|----------|-------------|
| **Azure SQL DB** | Near real-time | Dados operacionais SQL |
| **Snowflake** | Near real-time | Migra√ß√£o de Snowflake |
| **Azure Cosmos DB** | Near real-time | Dados NoSQL |

### 4.2 Configurando Mirroring

**Pr√©-requisitos**:
- Banco de origem deve ter CDC habilitado (SQL Server)
- Permiss√µes adequadas

**Passos**:
1. Criar Mirrored Database no Fabric
2. Configurar conex√£o com fonte
3. Selecionar tabelas para espelhar
4. Iniciar replica√ß√£o

**Exemplo - Azure SQL DB**:
```sql
-- Habilitar CDC no banco de origem
USE SourceDB;
EXEC sys.sp_cdc_enable_db;

-- Habilitar CDC em tabelas espec√≠ficas
EXEC sys.sp_cdc_enable_table
    @source_schema = 'dbo',
    @source_name = 'customers',
    @role_name = NULL;
```

### 4.3 Mirroring vs Shortcuts vs Pipelines

| Aspecto | Mirroring | Shortcuts | Pipelines |
|---------|-----------|-----------|-----------|
| **Lat√™ncia** | Near real-time | Real-time | Batch (agendado) |
| **Transforma√ß√£o** | N√£o | N√£o | Sim |
| **Fontes** | Bancos espec√≠ficos | Arquivos | Centenas de fontes |
| **Custo** | M√©dio | Baixo | Vari√°vel |
| **Complexidade** | Baixa | Baixa | M√©dia |

---

## 5. Transforma√ß√µes em Lote

### 5.1 Desnormaliza√ß√£o

**Quando usar**: Simplificar queries, melhorar performance.

```python
# Exemplo: Desnormalizar vendas com informa√ß√µes de produto e cliente
df_sales = spark.read.table("lakehouse.fact_sales")
df_product = spark.read.table("lakehouse.dim_product")
df_customer = spark.read.table("lakehouse.dim_customer")

df_denormalized = df_sales \
    .join(df_product, "product_id") \
    .join(df_customer, "customer_id") \
    .select(
        "sale_id",
        "sale_date",
        "customer_name",
        "customer_city",
        "product_name",
        "product_category",
        "quantity",
        "amount"
    )

df_denormalized.write \
    .format("delta") \
    .mode("overwrite") \
    .saveAsTable("lakehouse.sales_denormalized")
```

### 5.2 Agrega√ß√µes

```python
from pyspark.sql.functions import sum, avg, count, max as spark_max

# Agregar vendas por produto e m√™s
df_aggregated = df_sales.groupBy(
    "product_id",
    year("sale_date").alias("year"),
    month("sale_date").alias("month")
).agg(
    sum("amount").alias("total_sales"),
    avg("amount").alias("avg_sale"),
    count("*").alias("num_transactions"),
    spark_max("amount").alias("max_sale")
)

# Salvar como tabela agregada
df_aggregated.write \
    .format("delta") \
    .mode("overwrite") \
    .saveAsTable("lakehouse.sales_monthly_summary")
```

### 5.3 Tratamento de Duplicatas

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

# Identificar duplicatas usando ROW_NUMBER
window_spec = Window.partitionBy("customer_id", "sale_date").orderBy(col("created_at").desc())

df_with_row_num = df_sales.withColumn("row_num", row_number().over(window_spec))

# Manter apenas a primeira ocorr√™ncia
df_deduplicated = df_with_row_num.filter(col("row_num") == 1).drop("row_num")

# Alternativa: distinct (se todas as colunas devem ser √∫nicas)
df_distinct = df_sales.distinct()
```

### 5.4 Tratamento de Nulos

```python
from pyspark.sql.functions import coalesce, lit, when

# Preencher nulos com valor padr√£o
df_filled = df.fillna({
    "quantity": 0,
    "customer_name": "Unknown",
    "discount": 0.0
})

# Usar coalesce para substituir nulos
df_coalesced = df.withColumn(
    "email",
    coalesce(col("email"), lit("no-email@example.com"))
)

# L√≥gica condicional
df_conditional = df.withColumn(
    "status",
    when(col("amount") > 1000, "High Value")
    .when(col("amount") > 100, "Medium Value")
    .otherwise("Low Value")
)

# Remover linhas com nulos em colunas cr√≠ticas
df_no_nulls = df.dropna(subset=["customer_id", "product_id"])
```

### 5.5 Late-Arriving Data

**Estrat√©gia 1: Janela de Toler√¢ncia**

```python
# Processar dados dos √∫ltimos 7 dias (incluindo late arrivals)
from datetime import datetime, timedelta

lookback_days = 7
cutoff_date = (datetime.now() - timedelta(days=lookback_days)).strftime("%Y-%m-%d")

df_with_late = spark.read.table("lakehouse.bronze_sales") \
    .filter(col("sale_date") >= cutoff_date)

# MERGE para atualizar dados existentes
from delta.tables import DeltaTable

delta_table = DeltaTable.forName(spark, "lakehouse.silver_sales")

delta_table.alias("target").merge(
    df_with_late.alias("source"),
    "target.sale_id = source.sale_id"
).whenMatchedUpdateAll() \
 .whenNotMatchedInsertAll() \
 .execute()
```

---

## 6. Best Practices - Batch Processing

### ‚úÖ Performance

1. **Particionamento**:
   - Particione por coluna de filtro frequente
   - Cardinalidade ideal: 100-10.000 valores
   - Evite over-partitioning (muitas parti√ß√µes pequenas)

2. **Caching**:
   - Cache DataFrames usados m√∫ltiplas vezes
   - Libere cache quando n√£o precisar mais
   - Monitore uso de mem√≥ria

3. **Joins**:
   - Use broadcast para tabelas pequenas (< 10GB)
   - Particione ambas as tabelas pela chave de join
   - Evite cross joins

4. **Leitura/Escrita**:
   - Use column pruning (selecione apenas colunas necess√°rias)
   - Use predicate pushdown (filtre na leitura)
   - Escreva em formato Delta Lake (otimizado)

### ‚úÖ Escalabilidade

1. **Tamanho de Arquivos**:
   - Ideal: 128MB - 1GB por arquivo
   - Use OPTIMIZE para compactar arquivos pequenos
   - Use Z-ORDER para co-localizar dados relacionados

2. **Paralelismo**:
   - Configure n√∫mero de parti√ß√µes adequado
   - Use repartition antes de opera√ß√µes custosas
   - Monitore skew de dados

### ‚úÖ Confiabilidade

1. **Idempot√™ncia**:
   - Use MERGE em vez de INSERT
   - Suporte re-execu√ß√£o sem duplicatas
   - Mantenha logs de execu√ß√£o

2. **Qualidade de Dados**:
   - Valide dados na entrada
   - Trate nulos e duplicatas
   - Implemente data quality checks

---

## Refer√™ncias e Recursos

### Documenta√ß√£o Oficial

1. [Dataflows Gen2](https://learn.microsoft.com/fabric/data-factory/dataflows-gen2-overview)
2. [Mirroring Overview](https://learn.microsoft.com/fabric/database/mirroring/mirroring-overview)
3. [OneLake Shortcuts](https://learn.microsoft.com/fabric/onelake/onelake-shortcuts)
4. [Spark Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)
5. [Delta Lake Optimization](https://docs.delta.io/latest/optimizations-oss.html)
6. [Guia de Estudo DP-700](https://learn.microsoft.com/credentials/certifications/resources/study-guides/dp-700)

---

## Pontos-Chave para o Exame DP-700

üéØ **Memorize**:
- **Lakehouse**: Bronze/Silver, Delta Lake, Spark
- **Warehouse**: Gold, T-SQL, modelos dimensionais
- **Particionamento**: Cardinalidade m√©dia (100-10K valores)
- **Broadcast Join**: Tabela pequena (< 10GB)
- **Cache**: DataFrame usado m√∫ltiplas vezes
- **Shortcuts**: Refer√™ncia virtual, n√£o copia dados
- **Mirroring**: Near real-time, CDC-based, fontes espec√≠ficas
- **Predicate Pushdown**: Filtrar na leitura
- **Column Pruning**: Ler apenas colunas necess√°rias

üéØ **Entenda**:
- Quando usar Lakehouse vs Warehouse
- Como otimizar joins (broadcast, partitioning)
- Diferen√ßa entre Shortcuts, Mirroring e Pipelines
- Como tratar duplicatas (ROW_NUMBER)
- Como tratar nulos (fillna, coalesce)
- Estrat√©gias para late-arriving data
- Quando usar cache vs persist

