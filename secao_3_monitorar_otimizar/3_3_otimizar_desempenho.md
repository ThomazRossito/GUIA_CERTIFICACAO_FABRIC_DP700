# 3.3 Otimizar desempenho

A otimiza√ß√£o de desempenho √© o processo **cont√≠nuo e estrat√©gico** de ajustar e refinar sua solu√ß√£o de an√°lise para garantir que as consultas sejam r√°pidas, os processos de ingest√£o sejam eficientes e os custos de recursos sejam minimizados. O Microsoft Fabric oferece v√°rias alavancas de otimiza√ß√£o em seus diferentes componentes, desde a camada de armazenamento at√© o motor de processamento.

## Vis√£o Geral das Otimiza√ß√µes

```mermaid
graph TD
    A[Otimiza√ß√µes Fabric] --> B[Storage Layer]
    A --> C[Compute Layer]
    A --> D[Query Layer]
    
    B --> B1[V-Order]
    B --> B2[Z-Order]
    B --> B3[OPTIMIZE]
    B --> B4[Partitioning]
    
    C --> C1[Spark Pool Size]
    C --> C2[Caching]
    C --> C3[Parallelism]
    
    D --> D1[Statistics]
    D --> D2[DirectLake]
    D --> D3[Predicate Pushdown]
    
    style A fill:#e3f2fd
    style B fill:#e8f5e9
    style C fill:#fff4e1
    style D fill:#f3e5f5
```

---

## 1. Otimizar Tabelas Delta Lake

### 1.1 OPTIMIZE (Compacta√ß√£o)

**Problema**: Muitos arquivos pequenos degradam performance.

**Solu√ß√£o**: Compactar arquivos pequenos em arquivos maiores.

```sql
-- Compactar tabela inteira
OPTIMIZE lakehouse.sales;

-- Compactar parti√ß√£o espec√≠fica
OPTIMIZE lakehouse.sales
WHERE sale_date >= '2024-01-01';

-- Ver estat√≠sticas de otimiza√ß√£o
DESCRIBE HISTORY lakehouse.sales;
```

**Quando executar**:
- Ap√≥s m√∫ltiplas inser√ß√µes pequenas
- Ap√≥s streaming cont√≠nuo
- Quando n√∫mero de arquivos > 1000
- Quando tamanho m√©dio de arquivo < 128MB

**M√©tricas**:
```
Before OPTIMIZE:
- Files: 5,000
- Avg file size: 10 MB
- Query time: 45s

After OPTIMIZE:
- Files: 50
- Avg file size: 1 GB
- Query time: 8s (5.6x faster!)
```

### 1.2 V-Order

**O que √©**: Algoritmo propriet√°rio da Microsoft para otimizar layout de dados em Parquet.

**Benef√≠cios**:
- Compress√£o melhorada (~30-50%)
- Queries mais r√°pidas (~30-50%)
- Habilitado por padr√£o no Fabric

**Configura√ß√£o**:
```python
# V-Order √© habilitado automaticamente
# Para verificar se est√° ativo:
spark.conf.get("spark.sql.parquet.vorder.enabled")  # true

# Para desabilitar (n√£o recomendado):
spark.conf.set("spark.sql.parquet.vorder.enabled", "false")
```

**Quando V-Order √© aplicado**:
- Durante escrita de dados (INSERT, MERGE, CTAS)
- Durante OPTIMIZE
- N√£o √© aplicado em leituras

### 1.3 Z-Order

**O que √©**: T√©cnica de co-localiza√ß√£o de dados relacionados.

**Quando usar**:
- Colunas frequentemente usadas em filtros (WHERE)
- Colunas usadas em joins
- Colunas com cardinalidade m√©dia-alta

```sql
-- Z-Order por uma coluna
OPTIMIZE lakehouse.sales
ZORDER BY (customer_id);

-- Z-Order por m√∫ltiplas colunas (m√°ximo 4)
OPTIMIZE lakehouse.sales
ZORDER BY (sale_date, region);

-- Z-Order em parti√ß√£o espec√≠fica
OPTIMIZE lakehouse.sales
WHERE sale_date >= '2024-01-01'
ZORDER BY (customer_id);
```

**Compara√ß√£o de Performance**:

| Cen√°rio | Sem Z-Order | Com Z-Order | Melhoria |
|---------|-------------|-------------|----------|
| Query filtrada por customer_id | 30s | 5s | 6x |
| Query filtrada por sale_date | 25s | 4s | 6.25x |
| Query sem filtro | 40s | 38s | 5% |

**Limita√ß√µes**:
- M√°ximo 4 colunas
- N√£o funciona bem com colunas de baixa cardinalidade
- Requer re-execu√ß√£o peri√≥dica (dados novos n√£o s√£o Z-ordered)

### 1.4 VACUUM

**O que √©**: Remove arquivos antigos n√£o referenciados.

**Quando usar**:
- Ap√≥s m√∫ltiplos OPTIMIZE
- Ap√≥s muitas opera√ß√µes de UPDATE/DELETE
- Para economizar custos de armazenamento

```sql
-- VACUUM com reten√ß√£o padr√£o (7 dias)
VACUUM lakehouse.sales;

-- VACUUM com reten√ß√£o customizada
VACUUM lakehouse.sales RETAIN 168 HOURS;  -- 7 dias

-- Dry run (ver o que seria deletado)
VACUUM lakehouse.sales DRY RUN;
```

**‚ö†Ô∏è IMPORTANTE**:
- N√£o execute VACUUM com reten√ß√£o < 7 dias
- Time travel n√£o funciona para dados deletados
- Sempre fa√ßa backup antes

### 1.5 Particionamento

**Estrat√©gias de Particionamento**:

| Estrat√©gia | Quando Usar | Exemplo |
|------------|-------------|---------|
| **Por Data** | Dados com timestamp | `sale_date` |
| **Por Regi√£o** | Dados geogr√°ficos | `country`, `region` |
| **Por Categoria** | Dados categ√≥ricos | `product_category` |
| **H√≠brido** | Combina√ß√£o | `sale_date`, `region` |

**Implementa√ß√£o**:
```python
# Particionar ao escrever
df.write \
    .format("delta") \
    .partitionBy("sale_date", "region") \
    .mode("append") \
    .saveAsTable("lakehouse.sales")

# Verificar parti√ß√µes
spark.sql("SHOW PARTITIONS lakehouse.sales").show()
```

**Regras de Ouro**:
- Cardinalidade ideal: 100-10.000 parti√ß√µes
- Tamanho ideal por parti√ß√£o: 1GB
- Evite over-partitioning (muitas parti√ß√µes pequenas)
- Evite under-partitioning (poucas parti√ß√µes grandes)

---

## 2. Otimizar Pipelines

### 2.1 Copy Activity Performance

**Configura√ß√µes Cr√≠ticas**:

| Configura√ß√£o | Descri√ß√£o | Recomenda√ß√£o |
|--------------|-----------|--------------|
| **DIU (Data Integration Units)** | Unidades de compute | 4-256 (auto-scale) |
| **Parallel Copies** | Threads paralelas | 4-32 |
| **Degree of Copy Parallelism** | Paralelismo de c√≥pia | Auto |

**Exemplo de Configura√ß√£o**:
```json
{
  "source": {...},
  "sink": {...},
  "parallelCopies": 32,
  "dataIntegrationUnits": 256,
  "enableStaging": true,
  "stagingSettings": {
    "linkedServiceName": "ADLSGen2",
    "path": "staging/temp"
  }
}
```

### 2.2 Staged Copy

**Quando usar**:
- Fonte e destino em regi√µes diferentes
- Transforma√ß√µes complexas durante c√≥pia
- Melhorar throughput

```mermaid
graph LR
    A[Source DB] --> B[Staging ADLS]
    B --> C[Transformation]
    C --> D[Destination DW]
    
    style A fill:#e8f5e9
    style B fill:#fff4e1
    style D fill:#e3f2fd
```

### 2.3 Best Practices

1. **Filtrar na Fonte**:
```sql
-- BOM: Filtrar no SQL source
SELECT * FROM sales WHERE sale_date >= '2024-01-01'

-- RUIM: Copiar tudo e filtrar depois
```

2. **Usar Parti√ß√µes**:
- Habilitar partitioned copy
- Configurar partition column
- Ajustar n√∫mero de parti√ß√µes

3. **Monitorar Performance**:
- Verificar throughput (MB/s)
- Identificar gargalos (source vs sink)
- Ajustar DIU conforme necess√°rio

---

## 3. Otimizar Data Warehouse

### 3.1 Statistics

**Por que s√£o importantes**: Otimizador de queries usa statistics para criar planos eficientes.

**Criar Statistics**:
```sql
-- Criar statistics em coluna espec√≠fica
CREATE STATISTICS stats_customer_id 
ON dbo.Sales(customer_id);

-- Criar statistics em m√∫ltiplas colunas
CREATE STATISTICS stats_date_region 
ON dbo.Sales(sale_date, region);

-- Atualizar statistics
UPDATE STATISTICS dbo.Sales;

-- Ver statistics existentes
SELECT 
    s.name AS stats_name,
    c.name AS column_name,
    s.stats_date AS last_updated
FROM sys.stats s
JOIN sys.stats_columns sc ON s.stats_id = sc.stats_id
JOIN sys.columns c ON sc.column_id = c.column_id
WHERE s.object_id = OBJECT_ID('dbo.Sales');
```

**Quando criar**:
- Colunas em WHERE clauses
- Colunas em JOIN conditions
- Colunas em GROUP BY
- Ap√≥s cargas grandes de dados

### 3.2 Indexing

**Tipos de √çndices**:

| Tipo | Quando Usar | Performance |
|------|-------------|-------------|
| **Clustered Columnstore** | Tabelas grandes (> 60M rows) | Excelente para scans |
| **Clustered Index** | Tabelas pequenas, queries pontuais | Excelente para seeks |
| **Nonclustered Index** | Queries espec√≠ficas | Bom para filtros |

**Exemplo**:
```sql
-- Criar clustered columnstore (padr√£o no Fabric)
CREATE CLUSTERED COLUMNSTORE INDEX cci_sales 
ON dbo.Sales;

-- Criar nonclustered index
CREATE NONCLUSTERED INDEX idx_customer 
ON dbo.Sales(customer_id);
```

### 3.3 Materialized Views

**Quando usar**: Queries complexas executadas frequentemente.

```sql
-- Criar materialized view
CREATE MATERIALIZED VIEW mv_sales_summary AS
SELECT 
    sale_date,
    region,
    SUM(amount) AS total_sales,
    COUNT(*) AS num_transactions
FROM dbo.Sales
GROUP BY sale_date, region;

-- Refresh materialized view
ALTER MATERIALIZED VIEW mv_sales_summary REBUILD;
```

---

## 4. Otimizar Spark

### 4.1 Spark Pool Configuration

**Node Families**:

| Family | Quando Usar | Caracter√≠sticas |
|--------|-------------|-----------------|
| **Memory Optimized** | ETL, large datasets | Alta mem√≥ria/core |
| **Compute Optimized** | CPU-intensive | Alta CPU/core |

**Node Sizes**:
- Small: 4 cores, 32GB RAM
- Medium: 8 cores, 64GB RAM
- Large: 16 cores, 128GB RAM
- XLarge: 32 cores, 256GB RAM

**Autoscale**:
```
Min nodes: 3
Max nodes: 10
Auto-scale enabled: Yes
Dynamic allocation: Enabled
```

### 4.2 Caching Strategies

```python
# Cache DataFrame usado m√∫ltiplas vezes
df_customers = spark.read.table("lakehouse.customers")
df_customers.cache()

# Usar cached DataFrame
df_result1 = df_customers.filter(col("country") == "USA")
df_result2 = df_customers.filter(col("country") == "Brazil")

# Liberar cache quando n√£o precisar mais
df_customers.unpersist()

# Verificar DataFrames cacheados
# Spark UI ‚Üí Storage tab
```

### 4.3 Broadcast Joins

```python
from pyspark.sql.functions import broadcast

# Tabela grande
df_sales = spark.read.table("lakehouse.sales")  # 100M rows

# Tabela pequena
df_products = spark.read.table("lakehouse.products")  # 10K rows

# Broadcast join (muito mais r√°pido)
df_result = df_sales.join(
    broadcast(df_products),
    "product_id"
)

# Limite: Tabela broadcast < 10GB
```

### 4.4 Adaptive Query Execution (AQE)

**Habilitado por padr√£o no Fabric**.

**Benef√≠cios**:
- Ajusta plano de execu√ß√£o em runtime
- Otimiza joins automaticamente
- Reduz shuffle data

```python
# Verificar se AQE est√° habilitado
spark.conf.get("spark.sql.adaptive.enabled")  # true

# Configura√ß√µes AQE
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

---

## 5. Otimizar KQL Database

### 5.1 Ingestion Batching

**Configurar Batching Policy**:
```kql
.alter table events policy ingestionbatching 
@'{"MaximumBatchingTimeSpan":"00:01:00", "MaximumNumberOfItems": 10000, "MaximumRawDataSizeMB": 1024}'
```

**Trade-offs**:

| Configura√ß√£o | Lat√™ncia | Throughput | Custo |
|--------------|----------|------------|-------|
| Batch pequeno (30s) | Baixa | M√©dio | Alto |
| Batch m√©dio (5min) | M√©dia | Alto | M√©dio |
| Batch grande (30min) | Alta | Muito alto | Baixo |

### 5.2 Materialized Views

```kql
// Criar materialized view
.create materialized-view EventsHourly on table events
{
    events
    | summarize 
        count(), 
        avg(temperature), 
        max(temperature)
      by device_id, bin(timestamp, 1h)
}

// Queries usam automaticamente a view
events
| where timestamp > ago(24h)
| summarize avg(temperature) by device_id, bin(timestamp, 1h)
// Usa EventsHourly automaticamente!
```

### 5.3 Extent Management

```kql
// Ver extents (shards)
.show table events extents

// Merge extents pequenos
.alter-merge table events policy merge 
@'{"RowCountUpperBoundForMerge": 16000000}'
```

---

## 6. Otimizar Power BI

### 6.1 DirectLake Mode

**Benef√≠cios**:
- Performance de Import mode
- Atualiza√ß√£o de DirectQuery mode
- Sem duplica√ß√£o de dados

**Requisitos**:
- Dados em Delta Lake (Lakehouse)
- Workspace em capacidade Premium/Fabric
- Modelo sem√¢ntico compat√≠vel

**Configura√ß√£o**:
```
1. Criar semantic model do Lakehouse
2. Mode √© automaticamente DirectLake
3. Verificar: Model ‚Üí Settings ‚Üí Storage mode
```

**Limita√ß√µes**:
- N√£o suporta todas as transforma√ß√µes DAX
- Fallback para DirectQuery se necess√°rio
- Monitorar fallback rate

### 6.2 Aggregations

**Criar tabela agregada**:
```sql
CREATE TABLE sales_monthly AS
SELECT 
    YEAR(sale_date) AS year,
    MONTH(sale_date) AS month,
    region,
    SUM(amount) AS total_sales
FROM sales
GROUP BY YEAR(sale_date), MONTH(sale_date), region;
```

**Configurar no Power BI**:
1. Criar relationship entre sales e sales_monthly
2. Configurar aggregation mapping
3. Queries usam agrega√ß√£o automaticamente

---

## 7. Capacity Management

### 7.1 Monitorar Capacity

**Metrics App**:
- CU (Capacity Units) usage
- Throttling events
- Background operations
- Interactive operations

**Alertas**:
```
Configurar alertas quando:
- CU usage > 80%
- Throttling events > 10/hour
- Background operations > 100/hour
```

### 7.2 Otimizar Custos

**Estrat√©gias**:

1. **Pause quando n√£o usar**:
   - Pausar capacities em DEV/TEST fora do hor√°rio
   - Usar automation (Azure Functions)

2. **Right-size capacity**:
   - Monitorar usage patterns
   - Ajustar SKU conforme necess√°rio
   - Considerar autoscale

3. **Otimizar workloads**:
   - Executar jobs pesados off-peak
   - Usar incremental refresh
   - Limpar dados antigos (VACUUM)

---

## 8. Best Practices - Performance

### ‚úÖ Storage Layer

1. **OPTIMIZE regularmente**:
   - Schedule: Di√°rio ou semanal
   - Ap√≥s cargas grandes
   - Monitorar n√∫mero de arquivos

2. **Z-ORDER estrategicamente**:
   - M√°ximo 4 colunas
   - Colunas de filtro frequente
   - Re-executar periodicamente

3. **VACUUM periodicamente**:
   - Reten√ß√£o m√≠nima: 7 dias
   - Schedule: Semanal ou mensal
   - Monitorar storage costs

### ‚úÖ Compute Layer

1. **Right-size Spark pools**:
   - Usar autoscale
   - Monitorar utiliza√ß√£o
   - Ajustar conforme workload

2. **Cache inteligentemente**:
   - Apenas DataFrames reutilizados
   - Liberar cache n√£o usado
   - Monitorar mem√≥ria

3. **Broadcast joins**:
   - Tabelas < 10GB
   - Verificar auto-broadcast
   - For√ßar quando necess√°rio

### ‚úÖ Query Layer

1. **Statistics atualizadas**:
   - Criar em colunas cr√≠ticas
   - Atualizar ap√≥s cargas
   - Monitorar query plans

2. **DirectLake quando poss√≠vel**:
   - Usar para Lakehouses
   - Monitorar fallback rate
   - Otimizar modelo

---

## Refer√™ncias e Recursos

1. [Delta Lake Optimization](https://learn.microsoft.com/fabric/data-engineering/delta-optimization-and-v-order)
2. [Copy Activity Performance](https://learn.microsoft.com/azure/data-factory/copy-activity-performance)
3. [DirectLake Mode](https://learn.microsoft.com/power-bi/connect-data/desktop-directlake-overview)
4. [Spark Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)
5. [Guia de Estudo DP-700](https://learn.microsoft.com/credentials/certifications/resources/study-guides/dp-700)

---

## Pontos-Chave para o Exame DP-700

üéØ **Memorize**:
- **OPTIMIZE**: Compactar arquivos pequenos
- **V-Order**: Habilitado por padr√£o, melhora compress√£o e queries
- **Z-ORDER**: Co-localizar dados, m√°ximo 4 colunas
- **VACUUM**: Remover arquivos antigos, reten√ß√£o m√≠nima 7 dias
- **Particionamento**: Cardinalidade ideal 100-10K
- **Broadcast Join**: Tabelas < 10GB
- **DirectLake**: Performance de Import + atualiza√ß√£o de DirectQuery
- **Statistics**: Criar em colunas de filtro e join
- **DIU**: 4-256 para Copy Activity

üéØ **Entenda**:
- Quando usar OPTIMIZE vs Z-ORDER
- Trade-offs de particionamento
- Como V-Order funciona
- Diferen√ßa entre cache e persist
- Quando usar DirectLake vs Import vs DirectQuery
- Como statistics melhoram query plans
- Estrat√©gias de capacity management
- Otimiza√ß√µes de Spark (AQE, broadcast, cache)

